{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classification of anatomy on orthopedic X-ray data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage import data, exposure\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import exposure\n",
    "from skimage.filters import rank\n",
    "from skimage.morphology import disk\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Imbalance for only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/kaggle/input/mura12/MURA-v1.1/'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'valid')\n",
    "\n",
    "labels = ['XR_ELBOW','XR_FINGER','XR_FOREARM','XR_HAND','XR_HUMERUS','XR_SHOULDER','XR_WRIST']\n",
    "\n",
    "print(\"Train set:\\n========================================\")\n",
    "num_XR_ELBOW = len(os.listdir(os.path.join(train_dir, 'XR_ELBOW')))\n",
    "num_XR_FINGER = len(os.listdir(os.path.join(train_dir, 'XR_FINGER')))\n",
    "num_XR_FOREARM = len(os.listdir(os.path.join(train_dir, 'XR_FOREARM')))\n",
    "num_XR_HAND = len(os.listdir(os.path.join(train_dir, 'XR_HAND')))\n",
    "num_XR_HUMERUS = len(os.listdir(os.path.join(train_dir, 'XR_HUMERUS')))\n",
    "num_XR_SHOULDER = len(os.listdir(os.path.join(train_dir, 'XR_SHOULDER')))\n",
    "num_XR_WRIST = len(os.listdir(os.path.join(train_dir, 'XR_WRIST')))\n",
    "print(f\"XR_ELBOW={num_XR_ELBOW}\")\n",
    "print(f\"XR_XR_FINGER={num_XR_FINGER}\")\n",
    "print(f\"XR_FOREARM={num_XR_FOREARM}\")\n",
    "print(f\"XR_XR_HAND={num_XR_HAND}\")\n",
    "print(f\"XR_HUMERUS={num_XR_HUMERUS}\")\n",
    "print(f\"XR_XR_SHOULDER={num_XR_SHOULDER}\")\n",
    "print(f\"XR_XR_WRIST={num_XR_WRIST}\")\n",
    "\n",
    "print(\"Valid set:\\n========================================\")\n",
    "print(f\"XR_ELBOW={len(os.listdir(os.path.join(validation_dir, 'XR_ELBOW')))}\")\n",
    "print(f\"XR_XR_FINGER={len(os.listdir(os.path.join(validation_dir, 'XR_FINGER')))}\")\n",
    "print(f\"XR_FOREARM={len(os.listdir(os.path.join(validation_dir, 'XR_FOREARM')))}\")\n",
    "print(f\"XR_XR_HAND={len(os.listdir(os.path.join(validation_dir, 'XR_HAND')))}\")\n",
    "print(f\"XR_HUMERUS={len(os.listdir(os.path.join(validation_dir, 'XR_HUMERUS')))}\")\n",
    "print(f\"XR_XR_SHOULDER={len(os.listdir(os.path.join(validation_dir, 'XR_SHOULDER')))}\")\n",
    "print(f\"XR_XR_WRIST={len(os.listdir(os.path.join(validation_dir, 'XR_WRIST')))}\")\n",
    "\n",
    "\n",
    "weight_for_num_XR_ELBOW = num_XR_ELBOW / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\n",
    "weight_for_num_XR_FINGER = num_XR_FINGER / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\n",
    "weight_for_num_XR_FOREARM= num_XR_FOREARM / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\n",
    "weight_for_num_XR_HAND = num_XR_HAND / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\n",
    "weight_for_num_XR_HUMERUS = num_XR_HUMERUS / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\n",
    "weight_for_num_XR_SHOULDER = num_XR_SHOULDER / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\n",
    "weight_for_num_XR_WRIST = num_XR_WRIST / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\n",
    "\n",
    "\n",
    "\n",
    "class_weight = {0: weight_for_num_XR_ELBOW,\n",
    "                1: weight_for_num_XR_FINGER,\n",
    "                2: weight_for_num_XR_FOREARM,\n",
    "                3: weight_for_num_XR_HAND,\n",
    "                4:weight_for_num_XR_HUMERUS,\n",
    "                5: weight_for_num_XR_SHOULDER,\n",
    "                6: weight_for_num_XR_WRIST,\n",
    "                }\n",
    "\n",
    "print(f\"Weight for class weight_for_num_XR_ELBOW: {weight_for_num_XR_ELBOW:.2f}\")\n",
    "print(f\"Weight for class weight_for_num_XR_FINGER: {weight_for_num_XR_FINGER:.2f}\")\n",
    "print(f\"Weight for class weight_for_num_XR_FOREARM: {weight_for_num_XR_FOREARM:.2f}\")\n",
    "print(f\"Weight for class weight_for_num_XR_HAND: {weight_for_num_XR_HAND:.2f}\")\n",
    "print(f\"Weight for class weight_for_num_XR_HUMERUS: {weight_for_num_XR_HUMERUS:.2f}\")\n",
    "print(f\"Weight for class weight_for_num_XR_SHOULDER: {weight_for_num_XR_SHOULDER:.2f}\")\n",
    "print(f\"Weight for class weight_for_num_XR_WRIST: {weight_for_num_XR_WRIST:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implemented Densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def H(inputs, num_filters, dropout_rate):\n",
    "        x = tf.keras.layers.BatchNormalization(epsilon=eps)(inputs)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        x = tf.keras.layers.ZeroPadding2D((1, 1))(x)\n",
    "        x = tf.keras.layers.Conv2D(num_filters, kernel_size=(3, 3), use_bias=False, kernel_initializer='he_normal')(x)\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def transition(inputs, num_filters, compression_factor, dropout_rate):\n",
    "        x = tf.keras.layers.BatchNormalization(epsilon=eps)(inputs)\n",
    "        x = tf.keras.layers.Activation('relu')(x)\n",
    "        num_feature_maps = inputs.shape[1]  # The value of 'm'\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(np.floor(compression_factor * num_feature_maps).astype(np.int),\n",
    "                                   kernel_size=(1, 1), use_bias=False, padding='same', kernel_initializer='he_normal',\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2( 1e-4 ))(x)\n",
    "        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n",
    "\n",
    "        x = tf.keras.layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def dense_block(inputs, num_layers, num_filters, growth_rate, dropout_rate):\n",
    "        for i in range(num_layers):  # num_layers is the value of 'l'\n",
    "            conv_outputs = H(inputs, num_filters, dropout_rate)\n",
    "            inputs = tf.keras.layers.Concatenate()([conv_outputs, inputs])\n",
    "            num_filters += growth_rate  # To increase the number of filters for each layer.\n",
    "        return inputs, num_filters\n",
    "\n",
    "\n",
    "    input_shape = (224, 224, 3)\n",
    "    num_blocks = 3\n",
    "    num_layers_per_block = 4\n",
    "    growth_rate = 16\n",
    "    dropout_rate = 0\n",
    "    compress_factor = 0.5\n",
    "    eps = 1.1e-5\n",
    "\n",
    "    num_filters = 16\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "    x = tf.keras.layers.Conv2D(num_filters, kernel_size=(3, 3), use_bias=False, kernel_initializer='he_normal',\n",
    "                               kernel_regularizer=tf.keras.regularizers.l2( 1e-4 ))(inputs)\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "        x, num_filters = dense_block(x, num_layers_per_block, num_filters, growth_rate, dropout_rate)\n",
    "        x = transition(x, num_filters, compress_factor, dropout_rate)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(7)(x)\n",
    "    outputs = tf.keras.layers.Activation('softmax')(x)\n",
    "    model = tf.keras.models.Model(inputs, outputs)\n",
    "    model.summary()\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=3, mode='min')\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(lr=0.0007),\n",
    "                  metrics=['categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])  \n",
    "    dot_img_file = 'model_1.png'\n",
    "    tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Convolution network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "        # This is the first convolution\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        # The second convolution\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        # The third convolution\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        # The fourth convolution\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        # Flatten the results to feed into a DNN\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        # 512 neuron hidden layer\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(7, activation='softmax')])\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(lr=0.0007),\n",
    "                  metrics=['categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre train densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    vgg16 = tf.keras.applications.VGG16(\n",
    "    include_top=False, weights='imagenet',\n",
    "    input_shape=(224, 224, 3), pooling=None, classes=7,\n",
    "    classifier_activation='softmax'\n",
    "    )\n",
    "    \n",
    "    for layer in vgg16.layers:\n",
    "        layer.trainable = False\n",
    "    x = tf.keras.layers.Flatten()(vgg16.output)\n",
    "    x = tf.keras.layers.Dense(7, activation = 'softmax')(x) \n",
    "    model = tf.keras.models.Model(inputs = vgg16.input, outputs = x)\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=tf.keras.optimizers.Adam(lr=0.0007),\n",
    "                  metrics=['categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Saving the best weights\n",
    "When using 'Callback' and 'ModelCheckpoint' utilities of Keras, we can save the model with the best weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model during training \n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "scoreSeg = model.evaluate_generator(validation_generator, 400)\n",
    "\n",
    "save_at = \"/kaggle/working/model_Mura.hdf5\"\n",
    "save_best = ModelCheckpoint (save_at, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre train densenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  with strategy.scope():\n",
    "    model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    \n",
    "    x = model_d.output\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    preds = Dense(8, activation='softmax')(x)  # FC-layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Generater and complite model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data, exposure\n",
    "def AHE(img):\n",
    "    img_adapteq = exposure.equalize_adapthist(img, clip_limit=0.03)\n",
    "    \n",
    "    return img_adapteq\n",
    "def image(img):\n",
    "    return tf.image.per_image_standardization(img)\n",
    "    \n",
    "def equalization(img):\n",
    "    # some images are just one color, so they gerenate a divide by zero error\n",
    "    #     so return original image if the min and max values are the same\n",
    "    # print(\"image shape\",img.shape)\n",
    "    if (np.max(img) == np.min(img) ):\n",
    "        return img\n",
    "    # Equalization\n",
    "    img_equalized = exposure.equalize_hist(img)\n",
    "    return img_equalized\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    samplewise_center = True,\n",
    "    featurewise_std_normalization= False,\n",
    "    samplewise_std_normalization=True\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                  featurewise_std_normalization=False,\n",
    "                  samplewise_std_normalization=True,\n",
    "                  samplewise_center =True                                  )\n",
    "batch_size = 8\n",
    "# Flow training images in batches of 20 using train_datagen generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,  # This is the source directory for training images\n",
    "    target_size=(224, 224),  # All images will be resized to 150x150\n",
    "    batch_size=batch_size,\n",
    "    # Since we use binary_crossentropy loss, we need binary labels\n",
    "    class_mode='categorical',shuffle=False)\n",
    "print(\"train_generator\", train_generator.total_batches_seen)\n",
    "# Flow validation images in batches of 20 using test_datagen generator\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = next(validation_generator)\n",
    "Y_pred = np.round(model.predict(imgs))\n",
    "\n",
    "np.random.seed(87)\n",
    "for rand_num in np.random.randint(0, len(Y_pred), 5):\n",
    "    plt.figure()\n",
    "    plt.imshow(X_test[rand_num].reshape(100, 100)), plt.axis('off')\n",
    "    if np.where(Y_pred[rand_num] == 1)[0].sum() == np.where(Y_test[rand_num] == 1)[0].sum():\n",
    "        plt.title(encoder.classes_[np.where(Y_pred[rand_num] == 1)[0].sum()], color='g')\n",
    "    else :\n",
    "        plt.title(encoder.classes_[np.where(Y_pred[rand_num] == 1)[0].sum()], color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Images\n",
    "In this section, we verify the images are loading correctly, are readable and deliver reasonably understandable results. Batch size is 8 so 8 images are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = next(validation_generator)\n",
    "tg_class = {v:k for k,v in validation_generator.class_indices.items()}\n",
    "fig, m_axs = plt.subplots(3, 3, figsize = (12, 12))\n",
    "for c_ax, c_img, c_lab in zip(m_axs.flatten(), a, b):\n",
    "    c_ax.imshow(c_img[:,:,1], cmap = 'bone')\n",
    "    c_ax.axis('off')\n",
    "    c_ax.set_title(tg_class[np.argmax(c_lab)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 232\n",
    "# np.random.seed(seed)\n",
    "# tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agumented Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images_arr):\n",
    "    plt.figure()\n",
    "    fig, axes = plt.subplots(1,6, figsize=(40,40))\n",
    "\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "import numpy as np\n",
    "def plots(ims, figsize=(40,40), rows=1, interp=False, titles=None):\n",
    "    if type(ims[0]) is np.ndarray:\n",
    "        ims = np.array(ims).astype(np.uint8)\n",
    "        if (ims.shape[-1] != 3):\n",
    "            ims = ims.transpose((0,2,3,1))\n",
    "    f = plt.figure(figsize=figsize)\n",
    "    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n",
    "\n",
    "    for i in range(len(ims)):\n",
    "        sp = f.add_subplot(cols, rows, i+1)\n",
    "        sp.axis('Off')\n",
    "        if titles is not None:\n",
    "            sp.set_title(titles[i], fontsize=12)\n",
    "        plt.imshow(ims[i], interpolation=None if interp else 'none')\n",
    "#Check the training set (with batch of 10 as defined above\n",
    "imgs, labels = next(train_generator)\n",
    "print(\"train_generator.classes\",train_generator.classes)\n",
    "print(\"train_generator.indice\",train_generator.class_indices)\n",
    "#Images are shown in the output\n",
    "plots(imgs, titles=labels)\n",
    "\n",
    "augmented_images = [train_generator[1][1][1] for i in range(6)]\n",
    "plotImages(imgs)\n",
    "print(\"This is the label \\n\",labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "STEP_SIZE_TRAIN=math.ceil(train_generator.n / train_generator.batch_size)\n",
    "STEP_SIZE_VALID=math.ceil(validation_generator.n / validation_generator.batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=(STEP_SIZE_TRAIN),  # 2000 images = batch_size * steps\n",
    "    epochs=38,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=(STEP_SIZE_VALID),  # 1000 images = batch_size * steps\n",
    "    verbose=2,workers=1,use_multiprocessing=False,class_weight=class_weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will  plot 10 images at random from test set, but with titles as classified by the model, with every correct classification titled in 'green' color, and every incorrect classification titles in 'red' color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(87)\n",
    "\n",
    "X_test, labelsfromdata = next(validation_generator)\n",
    "Y_pred = np.round(model.predict(X_test))\n",
    "\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "labels=(validation_generator.class_indices)\n",
    "labels2=dict((v,k) for k,v in labels.items())\n",
    "print(labels2)\n",
    "\n",
    "for rand_num in np.random.randint(0, len(labels), 6):\n",
    "    plt.figure()\n",
    "    plt.imshow(X_test[rand_num]), plt.axis('off')\n",
    "    if np.where(Y_pred[rand_num] == 1)[0].sum() == np.where(labelsfromdata[rand_num] == 1)[0].sum():\n",
    "        plt.title(labels2[np.where(Y_pred[rand_num] == 1)[0].sum()], color='g')\n",
    "    else :\n",
    "        plt.title(labels2[np.where(Y_pred[rand_num] == 1)[0].sum()], color='r')\n",
    "        print('Correct Label',labels2[np.where(labelsfromdata[rand_num] == 1)[0].sum()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['XR_ELBOW','XR_FINGER','XR_FOREARM','XR_HAND','XR_HUMERUS','XR_SHOULDER','XR_WRIST']\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(\"Confusion Matrix for valid Set:\\n========================================\")\n",
    "validation_generator.reset()\n",
    "Y_pred = model.predict_generator(validation_generator, validation_generator.samples // batch_size+1)\n",
    "\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "labels=(validation_generator.class_indices)\n",
    "labels2=dict((v,k) for k,v in labels.items())\n",
    "\n",
    "predictions=[labels2[k] for k in y_pred]\n",
    "cf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "print(cf_matrix)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "plt.figure()\n",
    "print('Classification Report for Valid Set')\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=labels))\n",
    "print(\"Confusion Matrix for Train Set:\\n========================================\")\n",
    "train_generator.reset()\n",
    "Y_pred_T = model.predict_generator(train_generator, (train_generator.samples // batch_size+1))\n",
    "y_pred_T = np.argmax(Y_pred_T, axis=1)\n",
    "cf_matrix_T = confusion_matrix(train_generator.classes, y_pred_T)\n",
    "print(cf_matrix_T)\n",
    "\n",
    "\n",
    "print('Classification Report for Train Set')\n",
    "print(classification_report(train_generator.classes, y_pred_T, target_names=labels))\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "print(history.history.keys())\n",
    "acc=history.history['categorical_accuracy']\n",
    "\n",
    "val_acc=history.history['val_categorical_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "epochs=range(len(acc)) # Get number of epochs\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "# ------------------------------------------------\n",
    "plt.figure()\n",
    "plt.semilogy(epochs, acc, 'r', \"Training Accuracy\")\n",
    "plt.semilogy(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
    "plt.title('Training and validation Loss')\n",
    "plt.title('Model Accuracy', weight='bold', fontsize=16)\n",
    "plt.ylabel('accuracy', weight='bold', fontsize=14)\n",
    "plt.xlabel('epoch', weight='bold', fontsize=14)\n",
    "plt.grid(color = 'y', linewidth='0.5')\n",
    "plt.legend()\n",
    "# ------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "# ------------------------------------------------\n",
    "plt.figure()\n",
    "plt.semilogy(epochs, loss, 'r', \"Loss of Training\")\n",
    "plt.semilogy(epochs, val_loss, 'b', \"Loss of Validation\")\n",
    "plt.title('Training and validation Loss')\n",
    "plt.title('Loss of Training and Validation Set', weight='bold', fontsize=16)\n",
    "plt.ylabel('Loss ', weight='bold', fontsize=14)\n",
    "plt.grid(color = 'y', linewidth='0.5')\n",
    "plt.xlabel('epoch', weight='bold', fontsize=14)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
