{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Supervised classification of anatomy on orthopedic X-ray data"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport zipfile\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n# tf.logging.set_verbosity(tf.logging.ERROR)\n# devices = tf.config.experimental.list_physical_devices('GPU')\n# tf.config.experimental.set_memory_growth(devices[0], True)\nfrom skimage import data, exposure\nfrom tensorflow.compat.v1 import ConfigProto\nfrom tensorflow.compat.v1 import InteractiveSession\nimport numpy as np\nimport os\n# import matplotlib.pyplot as plt\nfrom matplotlib import pyplot as plt\nfrom skimage import exposure\nfrom skimage.filters import rank\nfrom skimage.morphology import disk\n\nconfig = ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = InteractiveSession(config=config)\nstrategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Class Imbalance for only"},{"metadata":{"trusted":true},"cell_type":"code","source":"base_dir = '/kaggle/input/mura12/MURA-v1.1/'\ntrain_dir = os.path.join(base_dir, 'train')\nvalidation_dir = os.path.join(base_dir, 'valid')\n\nlabels = ['XR_ELBOW','XR_FINGER','XR_FOREARM','XR_HAND','XR_HUMERUS','XR_SHOULDER','XR_WRIST']\n\nprint(\"Train set:\\n========================================\")\nnum_XR_ELBOW = len(os.listdir(os.path.join(train_dir, 'XR_ELBOW')))\nnum_XR_FINGER = len(os.listdir(os.path.join(train_dir, 'XR_FINGER')))\nnum_XR_FOREARM = len(os.listdir(os.path.join(train_dir, 'XR_FOREARM')))\nnum_XR_HAND = len(os.listdir(os.path.join(train_dir, 'XR_HAND')))\nnum_XR_HUMERUS = len(os.listdir(os.path.join(train_dir, 'XR_HUMERUS')))\nnum_XR_SHOULDER = len(os.listdir(os.path.join(train_dir, 'XR_SHOULDER')))\nnum_XR_WRIST = len(os.listdir(os.path.join(train_dir, 'XR_WRIST')))\nprint(f\"XR_ELBOW={num_XR_ELBOW}\")\nprint(f\"XR_XR_FINGER={num_XR_FINGER}\")\nprint(f\"XR_FOREARM={num_XR_FOREARM}\")\nprint(f\"XR_XR_HAND={num_XR_HAND}\")\nprint(f\"XR_HUMERUS={num_XR_HUMERUS}\")\nprint(f\"XR_XR_SHOULDER={num_XR_SHOULDER}\")\nprint(f\"XR_XR_WRIST={num_XR_WRIST}\")\n\nprint(\"Valid set:\\n========================================\")\nprint(f\"XR_ELBOW={len(os.listdir(os.path.join(validation_dir, 'XR_ELBOW')))}\")\nprint(f\"XR_XR_FINGER={len(os.listdir(os.path.join(validation_dir, 'XR_FINGER')))}\")\nprint(f\"XR_FOREARM={len(os.listdir(os.path.join(validation_dir, 'XR_FOREARM')))}\")\nprint(f\"XR_XR_HAND={len(os.listdir(os.path.join(validation_dir, 'XR_HAND')))}\")\nprint(f\"XR_HUMERUS={len(os.listdir(os.path.join(validation_dir, 'XR_HUMERUS')))}\")\nprint(f\"XR_XR_SHOULDER={len(os.listdir(os.path.join(validation_dir, 'XR_SHOULDER')))}\")\nprint(f\"XR_XR_WRIST={len(os.listdir(os.path.join(validation_dir, 'XR_WRIST')))}\")\n\n\nweight_for_num_XR_ELBOW = num_XR_ELBOW / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\nweight_for_num_XR_FINGER = num_XR_FINGER / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\nweight_for_num_XR_FOREARM= num_XR_FOREARM / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\nweight_for_num_XR_HAND = num_XR_HAND / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\nweight_for_num_XR_HUMERUS = num_XR_HUMERUS / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\nweight_for_num_XR_SHOULDER = num_XR_SHOULDER / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\nweight_for_num_XR_WRIST = num_XR_WRIST / (num_XR_ELBOW + num_XR_FINGER + num_XR_FOREARM + num_XR_HAND + num_XR_HUMERUS + num_XR_SHOULDER + num_XR_ELBOW)\n\n\n\nclass_weight = {0: weight_for_num_XR_ELBOW,\n                1: weight_for_num_XR_FINGER,\n                2: weight_for_num_XR_FOREARM,\n                3: weight_for_num_XR_HAND,\n                4:weight_for_num_XR_HUMERUS,\n                5: weight_for_num_XR_SHOULDER,\n                6: weight_for_num_XR_WRIST,\n                }\n\nprint(f\"Weight for class weight_for_num_XR_ELBOW: {weight_for_num_XR_ELBOW:.2f}\")\nprint(f\"Weight for class weight_for_num_XR_FINGER: {weight_for_num_XR_FINGER:.2f}\")\nprint(f\"Weight for class weight_for_num_XR_FOREARM: {weight_for_num_XR_FOREARM:.2f}\")\nprint(f\"Weight for class weight_for_num_XR_HAND: {weight_for_num_XR_HAND:.2f}\")\nprint(f\"Weight for class weight_for_num_XR_HUMERUS: {weight_for_num_XR_HUMERUS:.2f}\")\nprint(f\"Weight for class weight_for_num_XR_SHOULDER: {weight_for_num_XR_SHOULDER:.2f}\")\nprint(f\"Weight for class weight_for_num_XR_WRIST: {weight_for_num_XR_WRIST:.2f}\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Implemented Densenet"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    def H(inputs, num_filters, dropout_rate):\n        x = tf.keras.layers.BatchNormalization(epsilon=eps)(inputs)\n        x = tf.keras.layers.Activation('relu')(x)\n        x = tf.keras.layers.ZeroPadding2D((1, 1))(x)\n        x = tf.keras.layers.Conv2D(num_filters, kernel_size=(3, 3), use_bias=False, kernel_initializer='he_normal')(x)\n        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n        return x\n\n\n    def transition(inputs, num_filters, compression_factor, dropout_rate):\n        # compression_factor is the 'Î¸'\n        x = tf.keras.layers.BatchNormalization(epsilon=eps)(inputs)\n        x = tf.keras.layers.Activation('relu')(x)\n        num_feature_maps = inputs.shape[1]  # The value of 'm'\n\n        x = tf.keras.layers.Conv2D(np.floor(compression_factor * num_feature_maps).astype(np.int),\n                                   kernel_size=(1, 1), use_bias=False, padding='same', kernel_initializer='he_normal',\n                                   kernel_regularizer=tf.keras.regularizers.l2( 1e-4 ))(x)\n        x = tf.keras.layers.Dropout(rate=dropout_rate)(x)\n\n        x = tf.keras.layers.AveragePooling2D(pool_size=(2, 2))(x)\n        return x\n\n\n    def dense_block(inputs, num_layers, num_filters, growth_rate, dropout_rate):\n        for i in range(num_layers):  # num_layers is the value of 'l'\n            conv_outputs = H(inputs, num_filters, dropout_rate)\n            inputs = tf.keras.layers.Concatenate()([conv_outputs, inputs])\n            num_filters += growth_rate  # To increase the number of filters for each layer.\n        return inputs, num_filters\n\n\n    input_shape = (224, 224, 3)\n    num_blocks = 3\n    num_layers_per_block = 4\n    growth_rate = 16\n#     dropout_rate = 0.05\n    dropout_rate = 0\n    compress_factor = 0.5\n    eps = 1.1e-5\n\n    num_filters = 16\n\n    inputs = tf.keras.layers.Input(shape=input_shape)\n    x = tf.keras.layers.Conv2D(num_filters, kernel_size=(3, 3), use_bias=False, kernel_initializer='he_normal',\n                               kernel_regularizer=tf.keras.regularizers.l2( 1e-4 ))(inputs)\n\n    for i in range(num_blocks):\n        x, num_filters = dense_block(x, num_layers_per_block, num_filters, growth_rate, dropout_rate)\n        x = transition(x, num_filters, compress_factor, dropout_rate)\n\n    x = tf.keras.layers.Flatten()(x)\n\n    x = tf.keras.layers.Dense(512, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    x = tf.keras.layers.Dense(256, activation='relu')(x)\n    x = tf.keras.layers.Dense(7)(x)\n    outputs = tf.keras.layers.Activation('softmax')(x)\n    model = tf.keras.models.Model(inputs, outputs)\n    model.summary()\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',patience=3, mode='min')\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=tf.keras.optimizers.Adam(lr=0.0007),\n                  metrics=['categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])  \n    dot_img_file = 'model_1.png'\n    tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Normal Convolution network"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = tf.keras.models.Sequential([\n        # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n        # This is the first convolution\n        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n        tf.keras.layers.MaxPooling2D(2, 2),\n        # The second convolution\n        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n        tf.keras.layers.MaxPooling2D(2, 2),\n        # The third convolution\n        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n        tf.keras.layers.MaxPooling2D(2, 2),\n        # The fourth convolution\n        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n        tf.keras.layers.MaxPooling2D(2, 2),\n        # Flatten the results to feed into a DNN\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dropout(0.5),\n        # 512 neuron hidden layer\n        tf.keras.layers.Dense(512, activation='relu'),\n        tf.keras.layers.Dense(7, activation='softmax')])\n    model.summary()\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=tf.keras.optimizers.Adam(lr=0.0007),\n                  metrics=['categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pre train densenet"},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    vgg16 = tf.keras.applications.VGG16(\n    include_top=False, weights='imagenet',\n    input_shape=(224, 224, 3), pooling=None, classes=7,\n    classifier_activation='softmax'\n    )\n    \n    for layer in vgg16.layers:\n        layer.trainable = False\n    x = tf.keras.layers.Flatten()(vgg16.output)\n    x = tf.keras.layers.Dense(7, activation = 'softmax')(x) \n    model = tf.keras.models.Model(inputs = vgg16.input, outputs = x)\n    model.summary()\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=tf.keras.optimizers.Adam(lr=0.0007),\n                  metrics=['categorical_accuracy', tf.keras.metrics.AUC(), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#  Saving the best weights\nWhen using 'Callback' and 'ModelCheckpoint' utilities of Keras, we can save the model with the best weight"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save the model during training \nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import load_model\n\n\n\nscoreSeg = model.evaluate_generator(validation_generator, 400)\n\nsave_at = \"/kaggle/working/model_Mura.hdf5\"\nsave_best = ModelCheckpoint (save_at, monitor='val_accuracy', verbose=0, save_best_only=True, save_weights_only=False, mode='max')\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Pre train densenet"},{"metadata":{"trusted":true},"cell_type":"code","source":"  # with strategy.scope():\n    # model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n    #\n    # x = model_d.output\n    #\n    # x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    # x = tf.keras.layers.BatchNormalization()(x)\n    # # x = Dropout(0.5)(x)\n    # x = tf.keras.layers.Dense(1024, activation='relu')(x)\n    # x = tf.keras.layers.Dense(512, activation='relu')(x)\n    # x = tf.keras.layers.BatchNormalization()(x)\n    # x = Dropout(0.5)(x)\n\n    # preds = Dense(8, activation='softmax')(x)  # FC-layer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Generater and complite model "},{"metadata":{"trusted":true},"cell_type":"code","source":"from skimage import data, exposure\ndef AHE(img):\n    img_adapteq = exposure.equalize_adapthist(img, clip_limit=0.03)\n    \n    return img_adapteq\ndef image(img):\n    return tf.image.per_image_standardization(img)\n    \ndef equalization(img):\n    # some images are just one color, so they gerenate a divide by zero error\n    #     so return original image if the min and max values are the same\n    # print(\"image shape\",img.shape)\n    if (np.max(img) == np.min(img) ):\n        return img\n    # Equalization\n    img_equalized = exposure.equalize_hist(img)\n    return img_equalized\n\ntrain_datagen = ImageDataGenerator(\n# preprocessing_function=image,\n    rescale=1. / 255,\n#     rotation_range=10,\n#     zoom_range=0.1,\n#     horizontal_flip=True,\n    # zca_whitening = True,\n#     fill_mode='nearest',\n#     shear_range=0.3,\n    samplewise_center = True,\n#     # samplewise_std_normalization=True\n    featurewise_std_normalization= False,\n    samplewise_std_normalization=True\n)\ntest_datagen = ImageDataGenerator(rescale=1. / 255,\n#                     preprocessing_function=image,\n                  # horizontal_flip = True,\n                  # zoom_range = 0.0,\n#                     shear_range=0.3,\n#                   fill_mode='nearest',\n                  featurewise_std_normalization=False,\n                  samplewise_std_normalization=True,\n                  samplewise_center =True                                  )\nbatch_size = 8\n# Flow training images in batches of 20 using train_datagen generator\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,  # This is the source directory for training images\n    target_size=(224, 224),  # All images will be resized to 150x150\n    batch_size=batch_size,\n    # Since we use binary_crossentropy loss, we need binary labels\n    class_mode='categorical',shuffle=False)\nprint(\"train_generator\", train_generator.total_batches_seen)\n# Flow validation images in batches of 20 using test_datagen generator\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_dir,\n    target_size=(224, 224),\n    batch_size=batch_size,\n    class_mode='categorical',shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# imgs, labels = next(validation_generator)\n# # Y_pred = np.round(model.predict_generator(validation_generator, validation_generator.samples // batch_size+1))\n# Y_pred = np.round(model.predict(imgs))\n\n# np.random.seed(87)\n# for rand_num in np.random.randint(0, len(Y_pred), 5):\n#   plt.figure()\n#   plt.imshow(X_test[rand_num].reshape(100, 100)), plt.axis('off')\n#   if np.where(Y_pred[rand_num] == 1)[0].sum() == np.where(Y_test[rand_num] == 1)[0].sum():\n#     plt.title(encoder.classes_[np.where(Y_pred[rand_num] == 1)[0].sum()], color='g')\n#   else :\n#     plt.title(encoder.classes_[np.where(Y_pred[rand_num] == 1)[0].sum()], color='r')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Verify Images\nIn this section, we verify the images are loading correctly, are readable and deliver reasonably understandable results. Batch size is 8 so 8 images are displayed"},{"metadata":{"trusted":true},"cell_type":"code","source":"a, b = next(validation_generator)\ntg_class = {v:k for k,v in validation_generator.class_indices.items()}\nfig, m_axs = plt.subplots(3, 3, figsize = (12, 12))\nfor c_ax, c_img, c_lab in zip(m_axs.flatten(), a, b):\n    c_ax.imshow(c_img[:,:,1], cmap = 'bone')\n    c_ax.axis('off')\n    c_ax.set_title(tg_class[np.argmax(c_lab)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# # train_datagen = ImageDataGenerator(\n# # # preprocessing_function=equalization,\n# #     rescale=1. / 255,\n# #     rotation_range=10,\n# #     zoom_range=0.1,\n# #     horizontal_flip=True,\n# #     # zca_whitening = True,\n# #     fill_mode='nearest',\n# #     shear_range=0.3,\n# #     samplewise_center = True,\n# #     # samplewise_std_normalization=True\n# #     featurewise_std_normalization= False,\n# #     samplewise_std_normalization=True\n# # )\n\n# train_datagen = ImageDataGenerator(\n# # preprocessing_function=equalization,\n#     rescale=1. / 255,\n# #     rotation_range=10,\n# #     zoom_range=0.1,\n# #     horizontal_flip=True,\n#     # zca_whitening = True,\n#     fill_mode='nearest',\n#     shear_range=0.3,\n#     samplewise_center = True,\n#     # samplewise_std_normalization=True\n#     featurewise_std_normalization= False,\n#     samplewise_std_normalization=True\n# )\n# test_datagen = ImageDataGenerator(rescale=1. / 255,\n#                     # preprocessing_function=equalization,\n#                   # horizontal_flip = True,\n#                   # zoom_range = 0.0,\n#                     shear_range=0.3,\n#                   fill_mode='nearest',\n#                   featurewise_std_normalization=False,\n#                   samplewise_std_normalization=True,\n#                   samplewise_center =True                                  )\n# batch_size = 8\n# # Flow training images in batches of 20 using train_datagen generator\n# train_generator = train_datagen.flow_from_directory(\n#     train_dir,  # This is the source directory for training images\n#     target_size=(224, 224),  # All images will be resized to 150x150\n#     batch_size=batch_size,\n#     # Since we use binary_crossentropy loss, we need binary labels\n#     class_mode='categorical',shuffle=False)\n# print(\"train_generator\", train_generator.total_batches_seen)\n# # Flow validation images in batches of 20 using test_datagen generator\n# validation_generator = test_datagen.flow_from_directory(\n#     validation_dir,\n#     target_size=(224, 224),\n#     batch_size=batch_size,\n#     class_mode='categorical',shuffle=False)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting seeds for reproducibility"},{"metadata":{"trusted":true},"cell_type":"code","source":"# seed = 232\n# np.random.seed(seed)\n# tf.random.set_seed(seed)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Agumented Image"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotImages(images_arr):\n    plt.figure()\n    fig, axes = plt.subplots(1,6, figsize=(40,40))\n\n    axes = axes.flatten()\n    for img, ax in zip(images_arr, axes):\n        ax.imshow(img)\n    plt.tight_layout()\n    plt.show()\n\nimport numpy as np\ndef plots(ims, figsize=(40,40), rows=1, interp=False, titles=None):\n    if type(ims[0]) is np.ndarray:\n        ims = np.array(ims).astype(np.uint8)\n        if (ims.shape[-1] != 3):\n            ims = ims.transpose((0,2,3,1))\n    f = plt.figure(figsize=figsize)\n    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n\n    for i in range(len(ims)):\n        sp = f.add_subplot(cols, rows, i+1)\n        sp.axis('Off')\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=12)\n        plt.imshow(ims[i], interpolation=None if interp else 'none')\n#Check the training set (with batch of 10 as defined above\nimgs, labels = next(train_generator)\nprint(\"train_generator.classes\",train_generator.classes)\nprint(\"train_generator.indice\",train_generator.class_indices)\n#Images are shown in the output\nplots(imgs, titles=labels)\n\naugmented_images = [train_generator[1][1][1] for i in range(6)]\nplotImages(imgs)\nprint(\"This is the label \\n\",labels)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Running the simulation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import math\nSTEP_SIZE_TRAIN=math.ceil(train_generator.n / train_generator.batch_size)\nSTEP_SIZE_VALID=math.ceil(validation_generator.n / validation_generator.batch_size)\n\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch=(STEP_SIZE_TRAIN),  # 2000 images = batch_size * steps\n#     epochs=38,\n    epochs=38,\n    validation_data=validation_generator,\n    validation_steps=(STEP_SIZE_VALID),  # 1000 images = batch_size * steps\n    verbose=2,workers=1,use_multiprocessing=False,class_weight=class_weight)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will  plot 10 images at random from test set, but with titles as classified by the model, with every correct classification titled in 'green' color, and every incorrect classification titles in 'red' color."},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(87)\n\nX_test, labelsfromdata = next(validation_generator)\n# Y_pred = np.round(model.predict_generator(validation_generator, validation_generator.samples // batch_size+1))\nY_pred = np.round(model.predict(X_test))\n\ny_pred = np.argmax(Y_pred, axis=1)\nlabels=(validation_generator.class_indices)\nlabels2=dict((v,k) for k,v in labels.items())\nprint(labels2)\n# print(labels2[np.where(Y_pred[rand_num] == 1)[0].sum()])\n# print(np.where(labelsa[rand_num] == 1))\n# predictions=[labels2[k] for k in y_pred]\n\nfor rand_num in np.random.randint(0, len(labels), 6):\n    plt.figure()\n    plt.imshow(X_test[rand_num]), plt.axis('off')\n    if np.where(Y_pred[rand_num] == 1)[0].sum() == np.where(labelsfromdata[rand_num] == 1)[0].sum():\n        plt.title(labels2[np.where(Y_pred[rand_num] == 1)[0].sum()], color='g')\n    else :\n        plt.title(labels2[np.where(Y_pred[rand_num] == 1)[0].sum()], color='r')\n        print('Correct Label',labels2[np.where(labelsfromdata[rand_num] == 1)[0].sum()])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Check Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"labels = ['XR_ELBOW','XR_FINGER','XR_FOREARM','XR_HAND','XR_HUMERUS','XR_SHOULDER','XR_WRIST']\nimport numpy as np\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(\"Confusion Matrix for valid Set:\\n========================================\")\nvalidation_generator.reset()\nY_pred = model.predict_generator(validation_generator, validation_generator.samples // batch_size+1)\n\n# # Y_pred = model.predict(validation_generator.image_data_generator, batch_size= batch_size)\ny_pred = np.argmax(Y_pred, axis=1)\nlabels=(validation_generator.class_indices)\nlabels2=dict((v,k) for k,v in labels.items())\n\npredictions=[labels2[k] for k in y_pred]\n# print('Label 2',predictions)\ncf_matrix = confusion_matrix(validation_generator.classes, y_pred)\nprint(cf_matrix)\n\n\nimport seaborn as sns\nplt.figure()\n# sns_plot = sns.heatmap(cf_matrix, cmap='Blues')\n# fig = sns_plot.get_figure()\nprint('Classification Report for Valid Set')\nprint(classification_report(validation_generator.classes, y_pred, target_names=labels))\n#\nprint(\"Confusion Matrix for Train Set:\\n========================================\")\ntrain_generator.reset()\nY_pred_T = model.predict_generator(train_generator, (train_generator.samples // batch_size+1))\ny_pred_T = np.argmax(Y_pred_T, axis=1)\ncf_matrix_T = confusion_matrix(train_generator.classes, y_pred_T)\nprint(cf_matrix_T)\n\n# import seaborn as sns\n# plt.figure()\n# sns_plot_T= sns.heatmap(cf_matrix_T, annot=True,fmt='.2%', cmap='Blues')\n# fig_T = sns_plot_T.get_figure()\n\nprint('Classification Report for Train Set')\nprint(classification_report(train_generator.classes, y_pred_T, target_names=labels))\n\n# import matplotlib.image  as mpimg\n\n# plt.figure()\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nprint(history.history.keys())\nacc=history.history['categorical_accuracy']\n\nval_acc=history.history['val_categorical_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc)) # Get number of epochs\n\n# ------------------------------------------------\n# Plot training and validation accuracy per epoch\n# ------------------------------------------------\nplt.figure()\nplt.semilogy(epochs, acc, 'r', \"Training Accuracy\")\nplt.semilogy(epochs, val_acc, 'b', \"Validation Accuracy\")\nplt.title('Training and validation Loss')\nplt.title('Model Accuracy', weight='bold', fontsize=16)\nplt.ylabel('accuracy', weight='bold', fontsize=14)\nplt.xlabel('epoch', weight='bold', fontsize=14)\nplt.grid(color = 'y', linewidth='0.5')\nplt.legend()\n# ------------------------------------------------\n# Plot training and validation accuracy per epoch\n# ------------------------------------------------\nplt.figure()\nplt.semilogy(epochs, loss, 'r', \"Loss of Training\")\nplt.semilogy(epochs, val_loss, 'b', \"Loss of Validation\")\nplt.title('Training and validation Loss')\nplt.title('Loss of Training and Validation Set', weight='bold', fontsize=16)\nplt.ylabel('Loss ', weight='bold', fontsize=14)\nplt.grid(color = 'y', linewidth='0.5')\nplt.xlabel('epoch', weight='bold', fontsize=14)\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}